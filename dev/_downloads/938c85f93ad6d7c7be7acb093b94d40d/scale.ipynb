{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Scale pose tracks to real-world units\nConvert pixel coordinates to physical units using a known reference distance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom matplotlib import pyplot as plt\nfrom scipy.signal import find_peaks\n\nfrom movement import sample_data\nfrom movement.filtering import (\n    filter_by_confidence,\n    interpolate_over_time,\n    rolling_filter,\n)\nfrom movement.kinematics import compute_pairwise_distances\nfrom movement.transforms import scale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load sample dataset\nIn this example, we will use the ``DLC_single-mouse_DBTravelator_2D``\nsample dataset, which contains DeepLabCut predictions of a single\nmouse running across a dual-belt travelator, where the second belt\nruns faster than the first.\n\nThe back wall of the travelator has a visible 1 cm grid, which\nwe can use as a scaling reference to convert pixel coordinates into\nreal-world units.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ds = sample_data.fetch_dataset(\n    \"DLC_single-mouse_DBTravelator_2D.predictions.h5\", with_video=True\n)\nprint(ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see the DeepLabCut dataset contains positions and confidence scores\nfor 50 keypoints tracked on a single mouse, recorded at 247 fps over\napproximately 1.7 seconds.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare the dataset\nBefore scaling, let's inspect the tracked keypoints and clean up the\ndata.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that out of the 50 tracked keypoints, the majority track\npositions along the mouse's body (head, back, tail, and limbs).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(ds.keypoints.values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, there are also a number of keypoints which track physical\nlandmarks on the apparatus rather than the mouse itself. We don't need\nthese so we can filter them out of the dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "landmark_keypoints = [\n    \"Door\",\n    \"StartPlatL\",\n    \"StartPlatR\",\n    \"StepL\",\n    \"StepR\",\n    \"TransitionL\",\n    \"TransitionR\",\n]\n\n# Select all keypoints excluding the landmarks, and the single individual.\nds_mouse = ds.sel(\n    keypoints=~ds.keypoints.isin(landmark_keypoints),\n    individuals=\"individual_0\",\n)\n\nprint(ds_mouse)\nprint(\"----------------------------------\")\nprint(f\"Keypoints:\\n{ds_mouse.keypoints.values}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we remove low-confidence predictions, interpolate over gaps,\nand apply a rolling median filter to suppress any remaining tracking\noutliers.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ds_mouse[\"position\"] = filter_by_confidence(\n    ds_mouse.position,\n    ds_mouse.confidence,\n    threshold=0.9,\n)\nds_mouse[\"position\"] = interpolate_over_time(ds_mouse.position, max_gap=40)\nds_mouse[\"position\"] = rolling_filter(\n    ds_mouse.position,\n    window=6,\n    min_periods=2,\n    statistic=\"median\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualise the skeleton in pixels\nTo see what the pose data looks like before scaling, we define a helper\nfunction that draws the mouse skeleton in a single frame.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_skeleton(position_data, skeleton, frame, ax, s=3):\n    \"\"\"Plot the mouse skeleton for a single frame.\n\n    Parameters\n    ----------\n    position_data : xarray.DataArray\n        Position data with dimensions ``time``, ``keypoints``, and ``space``.\n    skeleton : list of tuple\n        List of (joint_1, joint_2) pairs defining skeletal connections,\n        where each element is a keypoint name present in ``position_data``.\n    frame : int\n        Index of the time frame to plot.\n    ax : matplotlib.axes.Axes\n        Axes on which to draw.\n    s : float, optional\n        Marker size for keypoint scatter points. Default is 3.\n\n    \"\"\"\n    pos_frame = position_data.squeeze().isel(time=frame)\n\n    # Draw skeleton connections\n    for joint_1, joint_2 in skeleton:\n        x1, y1 = pos_frame.sel(keypoints=joint_1)\n        x2, y2 = pos_frame.sel(keypoints=joint_2)\n        ax.plot([x1, x2], [y1, y2], color=\"b\", linewidth=1.5)\n\n    # Draw keypoints\n    for bodypart in pos_frame.keypoints:\n        x, y = pos_frame.sel(keypoints=bodypart)\n        ax.scatter(x, y, c=\"g\", s=s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define the connections between keypoints that form the mouse skeleton,\nthen plot it for a single frame using the above function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "skeleton = [\n    (\"Nose\", \"Back1\"),\n    (\"EarL\", \"Back1\"),\n    (\"EarR\", \"Back1\"),\n    (\"Back1\", \"Back2\"),\n    (\"Back2\", \"Back3\"),\n    (\"Back3\", \"Back4\"),\n    (\"Back4\", \"Back5\"),\n    (\"Back5\", \"Back6\"),\n    (\"Back6\", \"Back7\"),\n    (\"Back7\", \"Back8\"),\n    (\"Back8\", \"Back9\"),\n    (\"Back9\", \"Back10\"),\n    (\"Back10\", \"Back11\"),\n    (\"Back11\", \"Back12\"),\n    (\"Back12\", \"Tail1\"),\n    (\"Tail1\", \"Tail2\"),\n    (\"Tail2\", \"Tail3\"),\n    (\"Tail3\", \"Tail4\"),\n    (\"Tail4\", \"Tail5\"),\n    (\"Tail5\", \"Tail6\"),\n    (\"Tail6\", \"Tail7\"),\n    (\"Tail7\", \"Tail8\"),\n    (\"Tail8\", \"Tail9\"),\n    (\"Tail9\", \"Tail10\"),\n    (\"Tail10\", \"Tail11\"),\n    (\"Tail11\", \"Tail12\"),\n    (\"Back3\", \"ForepawKneeL\"),\n    (\"Back3\", \"ForepawKneeR\"),\n    (\"Back9\", \"HindpawKneeL\"),\n    (\"Back9\", \"HindpawKneeR\"),\n    (\"ForepawKneeL\", \"ForepawAnkleL\"),\n    (\"ForepawKneeR\", \"ForepawAnkleR\"),\n    (\"HindpawKneeL\", \"HindpawAnkleL\"),\n    (\"HindpawKneeR\", \"HindpawAnkleR\"),\n    (\"ForepawAnkleL\", \"ForepawKnuckleL\"),\n    (\"ForepawAnkleR\", \"ForepawKnuckleR\"),\n    (\"HindpawAnkleL\", \"HindpawKnuckleL\"),\n    (\"HindpawAnkleR\", \"HindpawKnuckleR\"),\n    (\"ForepawKnuckleL\", \"ForepawToeL\"),\n    (\"ForepawKnuckleR\", \"ForepawToeR\"),\n    (\"HindpawKnuckleL\", \"HindpawToeL\"),\n    (\"HindpawKnuckleR\", \"HindpawToeR\"),\n]\n\nexample_frame = 275\n\nfig, ax = plt.subplots(figsize=(8, 3))\nplot_skeleton(ds_mouse.position, skeleton, frame=example_frame, ax=ax, s=10)\n\nax.invert_yaxis()  # image coordinates have y increasing downward\nax.set_xlabel(\"x (pixels)\")\nax.set_ylabel(\"y (pixels)\")\nax.set_aspect(\"equal\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The skeleton gives us the shape of the mouse's body, but the coordinate\nsystem is in pixels. To interpret sizes and distances in real-world units,\nwe need to convert to physical units. We can do this by measuring a known\ndistance in the video using the ``napari`` GUI and then applying\n:func:`movement.transforms.scale` to convert from pixels to centimetres.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Measure a known distance from video footage\n.. attention::\n  The following steps require ``napari`` to be installed. If you haven't\n  already, install ``movement`` with the optional GUI dependencies by\n  following the `installation instructions<target-installation>`.\n\nFirst, open the video file in napari:\n\n```python\nimport napari\nviewer = napari.Viewer()\nviewer.open(ds_mouse.video_path)\n```\n<div class=\"alert alert-info\"><h4>Note</h4><p>You can also load a single frame image instead of the full video.</p></div>\n\nNext, measure a known distance in the napari viewer:\n\n1. Add a new shapes layer by clicking 'New shapes layer' in the layer list.\n2. Select the 'Add lines' tool (shortcut: L) from the layer controls.\n3. Draw a line across a feature of known length. In this case, the grid\n   squares are 1x1 cm, so we can draw a line along one side of a grid square.\n\n<img src=\"file://_static/napari_scale_draw.png\" width=\"600\">\n\n4. To read the line length in pixels, go to Layers \u2192 Measure \u2192 Toggle\nshape dimensions measurement (napari builtins). This displays P\n(perimeter) and A (area). In the case of a single line, the perimeter value\ncorresponds to the line length in pixels, and the area will always be 0.\n\n<img src=\"file://_static/napari_scale_measure.png\" width=\"600\">\n\n5. Close the napari viewer.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can then retrieve the measured distance and line coordinates from the\nshapes layer.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "shapes_layer = viewer.layers[\"Shapes\"]\nmeasurements_px = shapes_layer.features\nshape_coords = shapes_layer.data\n\nprint(f\"Measurements:\\n{measurements_px}\\n\")\nprint(f\"Coordinates:\\n{shape_coords}\")\n\n# Extract the line length in pixels from the perimeter (P) column\ndistance_px = measurements_px[\"_perimeter\"].values.squeeze()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transform poses to real units\nWe now transform the pose coordinates to real-world units in two steps:\nscaling from pixels to centimetres, and flipping the y-axis so that\npositive y corresponds to upward in real-world space (image coordinates\nhave the y-axis pointing downward).\n\nFirst, we calculate the scaling factor. The measured line spans one grid\nsquare, which we know to be 1 cm. Dividing this known length by the\ndistance in pixels gives us the scaling factor.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scaling_factor = 1 / distance_px\nprint(f\"Scaling factor: {scaling_factor:.6f} cm/pixel\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>This scaling factor assumes a constant relationship between pixels and\n   real-world units across the entire image, based on a single reference\n   measurement taken at the far side of the belt. In practice, several\n   factors can violate this assumption. For example, in 2D imaging,\n   perspective effects mean that the apparent size of objects varies with\n   their distance from the camera. Distances measured in the plane of the\n   reference grid will therefore be close to accurate, but can deviate\n   as the mouse moves away from this plane. Calibrated multi-camera\n   setups avoid these assumptions by mapping image coordinates directly\n   to real-world 3D space.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's inspect the position values before scaling.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Select a frame range in which the mouse is visible\nsample_range = np.arange(300, 301)\nprint(ds_mouse.position.isel(time=sample_range).values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we apply :func:`movement.transforms.scale` to convert from pixels to\ncentimetres. We can assign our space unit 'cm' here, to be stored as an\nattribute in ``xarray.DataArray.attrs['space_unit']``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ds_mouse[\"position\"] = scale(\n    ds_mouse[\"position\"], factor=scaling_factor, space_unit=\"cm\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we inspect our sample of values again. We can see the values have been\nadjusted.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(ds_mouse.position.isel(time=sample_range).values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The scaled data array's attributes now contain the ``space_unit`` and a\n``log`` entry recording the operation and its parameters, alongside the\noperations applied in earlier steps.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"Unit:\\n{ds_mouse['position'].space_unit}\\n\")\nprint(f\"Log:\\n{ds_mouse['position'].log}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we flip the y-axis by subtracting each y value from the maximum y\nvalue in the dataset. Note that the zero point in y is anchored to the\nlowest y position of the mouse in this recording, which approximates\nthe belt surface.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "y = ds_mouse[\"position\"].sel(space=\"y\")\nds_mouse[\"position\"].loc[dict(space=\"y\")] = y.max() - y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now re-plot the same skeleton, this time in centimetres and with\nthe y-axis pointing upward.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(8, 3))\nplot_skeleton(ds_mouse.position, skeleton, frame=example_frame, ax=ax, s=10)\n\nax.set_xlabel(\"x (cm)\")\nax.set_ylabel(\"y (cm)\")\nax.set_aspect(\"equal\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Working with real-world distances\nWith the data now in centimetres, we can measure the mouse directly. Let's\nfirst compute its approximate body length and height in ``example_frame``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "frame_x = ds_mouse.position.sel(space=\"x\").isel(time=example_frame)\nframe_y = ds_mouse.position.sel(space=\"y\").isel(time=example_frame)\n\nmouse_length = frame_x.sel(keypoints=\"Nose\") - frame_x.sel(keypoints=\"Tail12\")\nmouse_height = frame_y.max() - frame_y.min()\n\nprint(f\"Mouse length: {mouse_length.values:.1f} cm\")\nprint(f\"Mouse height: {mouse_height.values:.1f} cm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Beyond simple static measurements, real-world units also make aspects of\nmovement, such as gait-related distances interpretable. Let's visualise\nthe paw trajectories in x and y over time.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Select the toe keypoints for all four limbs\ntoe_keypoint_names = [\n    \"ForepawToeR\",\n    \"ForepawToeL\",\n    \"HindpawToeR\",\n    \"HindpawToeL\",\n]\n\n# Construct a new data array with only the toe keypoints\nds_limbs = ds_mouse.position.sel(keypoints=toe_keypoint_names)\n\nds_limbs.plot.line(\n    x=\"time\",\n    hue=\"keypoints\",\n    row=\"space\",\n    size=2.5,\n    aspect=2,\n    sharey=False,\n)\naxs = plt.gcf().axes\naxs[0].set_ylabel(\"x (cm)\")\naxs[1].set_xlabel(\"Time (s)\")\naxs[1].set_ylabel(\"y (cm)\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we see the mouse traverses approximately 60 cm across the travelator,\nand step heights typically remain below around 0.5 cm.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These trajectories give us a broad sense of limb movement, but we can go\nfurther and quantify dynamics between limbs, such as the inter-limb\ndistances in real-world units using\n:func:`movement.kinematics.compute_pairwise_distances`. Here, we again use\nthe toe keypoints to compare the anterior-posterior separation of the fore-\nand hindpaw pairs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "forepaw_dist = compute_pairwise_distances(\n    ds_mouse.position,\n    dim=\"keypoints\",\n    pairs={\"ForepawToeL\": \"ForepawToeR\"},\n)\nhindpaw_dist = compute_pairwise_distances(\n    ds_mouse.position,\n    dim=\"keypoints\",\n    pairs={\"HindpawToeL\": \"HindpawToeR\"},\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We plot these inter-limb distances for the fore- and hindpaws against the\nmean x-position of the body as the mouse traverses the travelator. Since\nwe know the first belt is 47 cm long, we can also easily plot the\ntransition point at which the mouse moves onto the faster second belt.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "belt_transition = 47  # cm\n\n# Compute a proxy for body centre in x\navg_body_x = ds_mouse.position.sel(space=\"x\").mean(dim=\"keypoints\")\n\nfig, ax = plt.subplots()\nax.plot(avg_body_x, forepaw_dist, color=\"tab:blue\", label=\"Forepaws\")\nax.plot(avg_body_x, hindpaw_dist, color=\"tab:orange\", label=\"Hindpaws\")\nax.axvline(belt_transition, color=\"k\", linestyle=\"--\", label=\"Belt transition\")\n\nax.set_xlabel(\"x (cm)\")\nax.set_ylabel(\"Inter-limb distance (cm)\")\nplt.legend()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The forepaw and hindpaw inter-limb distances oscillate largely in\nsynchrony, consistent with a trotting gait, with peak separations of\napproximately 4 cm for the majority of strides.\n\nHowever, after the mouse's body centre passes the belt transition,\nthe forepaw inter-limb distance drops noticeably whilst the hindpaw\ndistance increases. This likely reflects the mouse accommodating the speed\ndifference between belts - the forepaws shorten their stride to avoid\nover-reaching, while the hindpaws widen to keep the rear of the body from\ntrailing behind.\n\nWe can quantify these observations by comparing peak inter-limb distances\nacross all strides with those during only the transitioning stride.\n\nFirst, let's compute the mean peak inter-limb distance across all strides.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Find maximum interlimb distances in each stride by locating the peaks.\nforepaw_peaks, _ = find_peaks(forepaw_dist.values.squeeze(), prominence=0.2)\nhindpaw_peaks, _ = find_peaks(hindpaw_dist.values.squeeze(), prominence=0.2)\n\n# Find inter-limb distances at these peak locations\nforepaw_peak_vals = forepaw_dist.values.squeeze()[\n    forepaw_peaks[2:]\n]  # Exclude the first two peaks to match the available hindpaw strides\nhindpaw_peak_vals = hindpaw_dist.values.squeeze()[hindpaw_peaks]\n\nforepaw_mean = forepaw_peak_vals.mean()\nhindpaw_mean = hindpaw_peak_vals.mean()\n\nforepaw_std = forepaw_peak_vals.std()\nhindpaw_std = hindpaw_peak_vals.std()\n\npaw_diffs = hindpaw_peak_vals - forepaw_peak_vals\npaw_diffs_mean = np.mean(paw_diffs)\npaw_diffs_std = np.std(paw_diffs)\n\nprint(\n    f\"Mean peak inter-limb distance (forepaws): \"\n    f\"{forepaw_mean:.2f} \u00b1 {forepaw_std:.2f} cm (\u00b1 std)\"\n)\nprint(\n    f\"Mean peak inter-limb distance (hindpaws): \"\n    f\"{hindpaw_mean:.2f} \u00b1 {hindpaw_std:.2f} cm (\u00b1 std)\"\n)\nprint(\n    f\"Hindpaw - forepaw inter-limb difference: \"\n    f\"{paw_diffs_mean:.2f} \u00b1 {paw_diffs_std:.2f} cm (\u00b1 std)\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's compare these average distances with the maximum inter-limb\ndistances during the transitioning stride where the mouse steps onto the\nfaster second belt (the second-to-last peak in each trace).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "post_forepaw_dist = forepaw_dist.values.squeeze()[forepaw_peaks[-2]]\npost_hindpaw_dist = hindpaw_dist.values.squeeze()[hindpaw_peaks[-2]]\n\nprint(\n    f\"Transitioning forepaw inter-limb distance:  {post_forepaw_dist:.2f} cm\"\n)\nprint(\n    f\"Transitioning hindpaw inter-limb distance:  {post_hindpaw_dist:.2f} cm\"\n)\nprint(\n    f\"Hindpaw\u2013forepaw difference at transition:     \"\n    f\"{post_hindpaw_dist - post_forepaw_dist:.2f} cm\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we can see that during typical locomotion, the fore- and hindpaw\ninter-limb distances are synchronised and closely matched, differing by\nonly 0.31 cm on average. During the transitioning stride, however,\nthis difference grows more than tenfold, reflecting the gait adjustments\nthe mouse makes to accommodate the speed differential between belts.\n\nHaving the data in real-world units makes these quantitative comparisons\npossible.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}