{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load and visualise FreeMoCap 3D data\n\nLoad the ``.csv`` files from [FreeMoCap](https://freemocap.org) into\n``movement`` and visualise them in 3D.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports\nTo run this example, you will need to install the ``networkx`` package.\nYou can do this by running ``pip install networkx`` in your active\nvirtual environment (see [NetworkX's documentation](https://networkx.org/documentation/stable/install.html)\nfor more details).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport networkx as nx\nimport numpy as np\nimport pandas as pd\nimport xarray as xr\nfrom mpl_toolkits.mplot3d.art3d import Line3DCollection\n\nfrom movement import sample_data\nfrom movement.io import load_poses\nfrom movement.kinematics import compute_speed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load sample dataset\nIn this tutorial we demonstrate how to import 3D data\ncollected with\n[FreeMoCap](https://freemocap.org)\ninto ``movement``.\n\nFreeMoCap organises the collected data into timestamped ``session``\ndirectories. Each ``session`` directory typically holds multiple\n``recording`` subdirectories, each containing an ``output_data``\nsubdirectory. It is in this ``output_data`` subdirectory where FreeMoCap\nsaves the ``.csv`` files after each recording. Each file is named after\nthe model used to produce the data (e.g. ``face``, ``body``,\n``left_hand``, ``right_hand``). Each ``.csv`` file has 3N columns, with N\nbeing the number of keypoints used by the relevant model and 3 being the\nnumber of spatial dimensions (x, y, z).\n\nHere, we show how to load all output ``.csv`` files from a single FreeMoCap\nrecording into a unified ``movement`` dataset.\n\nTo do this, we use a sample FreeMoCap session folder with two\nrecordings. In the first recording, a human writes the word \"hello\" in the\nair with their index finger (``recording_15_37_37_gmt+1``).\nIn the second recording, they write the word \"world\"\n(``recording_15_45_49_gmt+1``).\n\nLet's first fetch the dataset using the ``sample_data`` module and verify\nthe folder structure.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "session_dir_path = sample_data.fetch_dataset_paths(\n    \"FreeMoCap_hello-world_session-folder.zip\"\n)[\"poses\"]\n\n# path to output data directory for \"hello\" recording\noutput_data_dir_hello = session_dir_path.joinpath(\n    \"recording_15_37_37_gmt+1/output_data\"\n)\n\n# path to output data directory for \"world\" recording\noutput_data_dir_world = session_dir_path.joinpath(\n    \"recording_15_45_49_gmt+1/output_data\"\n)\n\nprint(\"Path to session folder: \", session_dir_path)\nprint(\n    \"Path to output data directory for 'hello' recording: \",\n    output_data_dir_hello,\n)\nprint(\n    \"Path to output data directory for 'world' recording: \",\n    output_data_dir_world,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Read FreeMoCap output files as a single ``movement`` dataset\nWe use the helper function below to combine all FreeMoCap output\n``.csv`` files into one ``movement`` dataset. Since there is no confidence\ndata available in the output files, we set the confidence of all\nkeypoints to the default NaN value. We also use the default individual\nname ``id_0``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def read_freemocap_as_ds(output_data_dir):\n    \"\"\"Read FreeMoCap output files as a single ``movement`` dataset.\n\n    Parameters\n    ----------\n    output_data_dir : pathlib.Path\n        Path to the recording's output data directory holding the relevant\n        ``.csv`` files.\n\n    Returns\n    -------\n    xarray.Dataset\n        A ``movement`` dataset containing the data from all FreeMoCap output\n        files. The ``keypoints`` dimension will have the full set of keypoints\n        as coordinates. The ``individuals`` dimension will have a single\n        coordinate, ``id_0``. The confidence of all keypoints is set to\n        the default NaN value.\n\n    \"\"\"\n    list_models = [\"body\", \"left_hand\", \"right_hand\", \"face\"]\n    list_datasets = []\n    for m in list_models:\n        # Read .csv file as a pandas DataFrame\n        data_pd = pd.read_csv(\n            output_data_dir.joinpath(f\"mediapipe_{m}_3d_xyz.csv\")\n        )\n\n        # Get list of keypoints from the dataframe, preserving the order\n        # they appear in the file.\n        list_keypoints = [h[:-2] for h in data_pd.columns[::3]]\n\n        # Format data as numpy array\n        data = data_pd.to_numpy()\n        data = data.reshape(\n            data.shape[0], int(data.shape[1] / 3), 3\n        )  # time, keypoint, space\n\n        # Transpose to align with movement dimensions\n        # and add individuals dimension at the end\n        data = np.transpose(data, [0, 2, 1])[..., None]\n\n        # Read as a movement dataset\n        ds = load_poses.from_numpy(\n            position_array=data,  # time, space, keypoint, individual\n            keypoint_names=list_keypoints,\n        )\n        list_datasets.append(ds)\n\n    # Merge all datasets along keypoint dimension\n    ds_all_keypoints = xr.merge(list_datasets)\n    return ds_all_keypoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now use the helper function to read the files\nin each output directory as a ``movement`` dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ds_hello = read_freemocap_as_ds(output_data_dir_hello)\nds_world = read_freemocap_as_ds(output_data_dir_world)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that each ``movement`` dataset holds the data for\nall keypoints across all models used by FreeMoCap.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"Number of keypoints in 'hello' dataset: {len(ds_hello.keypoints)}\")\nprint(f\"Number of keypoints in 'world' dataset: {len(ds_world.keypoints)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualise a subset of the data\nWe would now like to visually inspect the loaded data. For clarity, we\nfocus on the specific time window when the motion takes place, and extract\nthe data for the single individual we have recorded and their\n``right_hand_0006`` keypoint. This keypoint tracks the right index finger\nused for writing in the air.  Note that at the moment, FreeMoCap can only\ntrack [one individual at a time](https://freemocap.github.io/documentation/frequently-asked-questions-faq.html#can-freemocap-track-multiple-people-at-once).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "right_index_position_hello = ds_hello.position.sel(time=range(30, 180)).sel(\n    keypoints=\"right_hand_0006\", individuals=\"id_0\"\n)\nright_index_position_world = ds_world.position.sel(time=range(150)).sel(\n    keypoints=\"right_hand_0006\", individuals=\"id_0\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use the following helper function for plotting.\nThis function is adapted from\n[matplotlib's multi-coloured line example](https://matplotlib.org/stable/gallery/lines_bars_and_markers/multicolored_line.html).\nYou don't need to understand how this function works in detail, for now it\nis enough to know that we will use it to plot 3D lines with a colour\ndetermined by a scalar.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def coloured_scatter_line_3d(x, y, z, c, s, ax, **lc_kwargs):\n    \"\"\"Plot a 3D line and colour by a scalar.\n\n    Parameters\n    ----------\n    x, y, z : array-like\n        x, y, z-coordinates of the line to plot.\n    c : array-like\n        Scalar valuesto colour the line by.\n    s : float\n        Size of the markers on the line.\n    ax : matplotlib.axes.Axes\n        Axes to plot the line on.\n    **lc_kwargs : dict\n        Keyword arguments to pass to ``Line3DCollection``.\n\n    Returns\n    -------\n    lc : matplotlib.collections.Line3DCollection\n        The line collection.\n\n    \"\"\"\n    ax.scatter(x, y, z, c=c, s=s, **lc_kwargs)\n\n    x, y, z = (np.asarray(arr).ravel() for arr in (x, y, z))\n    default_kwargs = {\"capstyle\": \"butt\"}\n    default_kwargs.update(lc_kwargs)\n\n    x_mid = np.hstack((x[:1], 0.5 * (x[1:] + x[:-1]), x[-1:]))\n    y_mid = np.hstack((y[:1], 0.5 * (y[1:] + y[:-1]), y[-1:]))\n    z_mid = np.hstack((z[:1], 0.5 * (z[1:] + z[:-1]), z[-1:]))\n\n    start = np.column_stack((x_mid[:-1], y_mid[:-1], z_mid[:-1]))[:, None, :]\n    mid = np.column_stack((x, y, z))[:, None, :]\n    end = np.column_stack((x_mid[1:], y_mid[1:], z_mid[1:]))[:, None, :]\n\n    segments = np.concatenate((start, mid, end), axis=1)\n\n    lc = Line3DCollection(segments, **default_kwargs)\n    lc.set_array(c)\n    ax.add_collection3d(lc)\n\n    return lc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use the above function to plot the trajectory of the right index\nfinger as it writes the words \"hello\" and \"world\". We colour each\npoint by the frame index of each recording within the time window of\ninterest.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\nax = fig.add_subplot(projection=\"3d\")\ncolour_map = \"turbo\"\n\n# plot right index finger for \"hello\" recording\ncoloured_scatter_line_3d(\n    right_index_position_hello.sel(space=\"x\"),\n    right_index_position_hello.sel(space=\"y\"),\n    right_index_position_hello.sel(space=\"z\"),\n    ax=ax,\n    c=right_index_position_world.time,\n    s=5,\n    cmap=colour_map,\n)\n\n# plot right index finger for \"world\" recording\n# (shifted down by 400 mm in the z-axis to avoid overlap with the \"hello\" data)\ncoloured_scatter_line_3d(\n    right_index_position_world.sel(space=\"x\"),\n    right_index_position_world.sel(space=\"y\"),\n    right_index_position_world.sel(space=\"z\") - 400,\n    ax=ax,\n    c=right_index_position_world.time,\n    s=5,\n    cmap=colour_map,\n)\n\nax.view_init(elev=-20, azim=137, roll=0)\nax.set_aspect(\"equal\")\nax.set_xlabel(\"x (mm)\")\nax.set_ylabel(\"y (mm)\")\nax.set_zlabel(\"z (mm)\")\ncb = fig.colorbar(\n    ax.collections[0],\n    ax=ax,\n    label=\"Frame index\",\n    orientation=\"horizontal\",\n    pad=-0.05,\n    fraction=0.06,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also visualise how the writing speed changes along the trajectory,\nusing the above plotting function and the\n:func:`movement.kinematics.compute_speed` function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\nax = fig.add_subplot(projection=\"3d\")\ncolour_map = \"inferno\"\n\n# Use movement helpers to compute speed\nspeed_hello = compute_speed(right_index_position_hello)\nspeed_world = compute_speed(right_index_position_world)\n\n# plot right index finger for \"hello\" recording\ncoloured_scatter_line_3d(\n    right_index_position_hello.sel(space=\"x\"),\n    right_index_position_hello.sel(space=\"y\"),\n    right_index_position_hello.sel(space=\"z\"),\n    ax=ax,\n    c=speed_hello,\n    s=5,\n    cmap=colour_map,\n)\n\n# plot right index finger for \"world\" recording\n# (shifted down by 400 mm in the z-axis to avoid overlap with the \"hello\" data)\ncoloured_scatter_line_3d(\n    right_index_position_world.sel(space=\"x\"),\n    right_index_position_world.sel(space=\"y\"),\n    right_index_position_world.sel(space=\"z\") - 400,\n    ax=ax,\n    c=speed_world,\n    s=5,\n    cmap=colour_map,\n)\n\nax.view_init(elev=-20, azim=137, roll=0)\nax.set_aspect(\"equal\")\nax.set_xlabel(\"x (mm)\")\nax.set_ylabel(\"y (mm)\")\nax.set_zlabel(\"z (mm)\")\ncb = fig.colorbar(\n    ax.collections[0],\n    ax=ax,\n    label=\"Speed (mm/s)\",\n    orientation=\"horizontal\",\n    pad=-0.05,\n    fraction=0.06,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the above plot, we can see that the speed is highest on long, straight\nstrokes and lowest around kinks.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualise the skeleton\nNext, we use the ``networkx`` package to define a graph based on a\nsubset of keypoints. This allows us to more easily plot a skeleton of\nthe upper-half of the individual.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Select some keypoints from the \"body\" model\nselected_body_kpts = [\n    \"body_nose\",\n    \"body_left_eye\",\n    \"body_right_eye\",\n    \"body_left_shoulder\",\n    \"body_right_shoulder\",\n    \"body_left_hip\",\n    \"body_right_hip\",\n    \"body_left_elbow\",\n    \"body_right_elbow\",\n    \"body_left_wrist\",\n    \"body_right_wrist\",\n    \"body_left_index\",\n    \"body_right_index\",\n    \"body_left_pinky\",\n    \"body_right_pinky\",\n    \"body_left_thumb\",\n    \"body_right_thumb\",\n    \"body_left_ear\",\n    \"body_right_ear\",\n    \"body_mouth_left\",\n    \"body_mouth_right\",\n]\n\n# Select a frame index to extract the positions of the keypoints\nbody_frame = 130\n\n# Initialize a graph\nG = nx.Graph()\n\n# Add nodes to the graph.\nfor kpt in selected_body_kpts:\n    G.add_node(\n        kpt,\n        position=ds_hello.position.sel(time=body_frame, keypoints=kpt).values,\n    )\n\n# Add midpoint between the shoulders as node\nG.add_node(\n    \"body_shoulders_midpoint\",\n    position=ds_hello.position.sel(\n        time=body_frame,\n        keypoints=[\"body_left_shoulder\", \"body_right_shoulder\"],\n    )\n    .mean(dim=\"keypoints\")\n    .values,\n)\n\n# Add edges to the graph.\nG.add_edge(\"body_right_shoulder\", \"body_left_shoulder\")\nfor side_str in [\"left\", \"right\"]:\n    G.add_edge(f\"body_{side_str}_shoulder\", f\"body_{side_str}_elbow\")\n    G.add_edge(f\"body_{side_str}_shoulder\", f\"body_{side_str}_hip\")\n    G.add_edge(f\"body_{side_str}_elbow\", f\"body_{side_str}_wrist\")\n    G.add_edge(f\"body_{side_str}_wrist\", f\"body_{side_str}_index\")\n    G.add_edge(f\"body_{side_str}_wrist\", f\"body_{side_str}_pinky\")\n    G.add_edge(f\"body_{side_str}_wrist\", f\"body_{side_str}_thumb\")\n    G.add_edge(f\"body_mouth_{side_str}\", f\"body_{side_str}_ear\")\n\n# Add edge between the shoulders midpoint and the nose\nG.add_edge(\"body_shoulders_midpoint\", \"body_nose\")\n\n# Add edge between the ears\nG.add_edge(\"body_left_ear\", \"body_right_ear\")\n\n# Add edge across the mouth\nG.add_edge(\"body_mouth_left\", \"body_mouth_right\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now use the above graph to plot the skeleton.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\nax = fig.add_subplot(111, projection=\"3d\")\n\n# Get dictionary of node positions\n# (key: node, value: (x, y, z))\npositions_dict = nx.get_node_attributes(G, \"position\")\n\n# Plot nodes of skeleton in green\nfor _node, (x, y, z) in positions_dict.items():\n    ax.scatter(x, y, z, s=10, color=\"green\")\n\n# Plot edges of skeleton in blue\nfor edge in G.edges():\n    node_1, node_2 = edge\n    x1, y1, z1 = positions_dict[node_1]\n    x2, y2, z2 = positions_dict[node_2]\n    ax.plot([x1, x2], [y1, y2], [z1, z2], \"b-\")\n\n# Plot the right index finger keypoint for the selected time window\n# in magenta\nax.plot(\n    right_index_position_hello.sel(space=\"x\"),\n    right_index_position_hello.sel(space=\"y\"),\n    right_index_position_hello.sel(space=\"z\"),\n    alpha=0.35,\n    color=\"magenta\",\n)\n\nax.view_init(elev=25, azim=-120, roll=0)\nax.set_aspect(\"equal\")\nax.set_xlabel(\"x (mm)\")\nax.set_ylabel(\"y (mm)\")\nax.set_zlabel(\"z (mm)\")\n\n# Invert x-axis to make traced text readable\nax.invert_xaxis()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}